{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39e9847a-3b5b-45f2-9677-aa9de03553b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://sbert.net/\n",
    "#https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "#https://huggingface.co/openai/whisper-large-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef7684c-0521-49f9-afa0-6838be7a58ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandonamarasingam/anaconda3/envs/pytorch/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd1cc23-e77c-4ea1-9a4e-949353e2bb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "828"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the list from the JSON file\n",
    "with open('speech_total.json', 'r') as file:\n",
    "    speech_total = json.load(file)\n",
    "\n",
    "len(speech_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c0a5c1c-dc7d-4d94-a81b-ba016864f686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandonamarasingam/anaconda3/envs/pytorch/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 384)\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Your text data\n",
    "texts = [\"Your first text here\", \"Your second text here\", \"And so on...\"]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(texts)\n",
    "\n",
    "# Now embeddings is a numpy array of shape (n_texts, embedding_dim)\n",
    "print(embeddings.shape)\n",
    "\n",
    "# You can now use these embeddings for further processing or storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a624e25-096a-4b7e-afff-f70dd2ec0c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pypi.org/project/faiss-cpu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba997998-3c0b-4d23-bab3-75157b77af9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances: [[0.        0.3136469]]\n",
      "Indices: [[0 1]]\n",
      "Nearest neighbor text: Your first text here\n",
      "Nearest neighbor text: Your second text here\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Assuming you have your embeddings in a variable called 'embeddings'\n",
    "# and your original texts in a list called 'texts'\n",
    "\n",
    "# Convert embeddings to a numpy array if it's not already\n",
    "embeddings_np = np.array(embeddings).astype('float32')\n",
    "\n",
    "# Get the dimension of the embeddings\n",
    "dimension = embeddings_np.shape[1]\n",
    "\n",
    "# Create a FAISS index\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add the vectors to the index\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# Optionally, you can save the index to a file\n",
    "faiss.write_index(index, \"faiss_index.bin\")\n",
    "\n",
    "# To demonstrate how to use the index, let's do a simple search\n",
    "# Let's say we want to find the 2 nearest neighbors for the first embedding\n",
    "k = 2\n",
    "D, I = index.search(embeddings_np[:1], k)\n",
    "\n",
    "print(f\"Distances: {D}\")\n",
    "print(f\"Indices: {I}\")\n",
    "\n",
    "# Print the original texts for the nearest neighbors\n",
    "for idx in I[0]:\n",
    "    print(f\"Nearest neighbor text: {texts[idx]}\")\n",
    "\n",
    "# If you need to load the index later, you can use:\n",
    "# loaded_index = faiss.read_index(\"faiss_index.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fb78a7e-a2bb-417b-a462-85ffbc420149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandonamarasingam/anaconda3/envs/pytorch/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'original_texts.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Load your original texts (assuming you've saved them somewhere)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moriginal_texts.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     23\u001b[0m     texts \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_context\u001b[39m(query, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Generate embedding for the query\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'original_texts.pkl'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import faiss\n",
    "\n",
    "# Load the embedding model\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load the language model and tokenizer\n",
    "model_name = \"distilgpt2\"  # A smaller model that can run on CPU\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Load your FAISS index (assuming you've already created it)\n",
    "index = faiss.read_index(\"faiss_index.bin\")\n",
    "\n",
    "# Load your original texts (assuming you've saved them somewhere)\n",
    "import pickle\n",
    "with open('original_texts.pkl', 'rb') as f:\n",
    "    texts = pickle.load(f)\n",
    "\n",
    "def retrieve_context(query, k=3):\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embed_model.encode([query])[0]\n",
    "    \n",
    "    # Search the FAISS index\n",
    "    D, I = index.search(np.array([query_embedding]).astype('float32'), k)\n",
    "    \n",
    "    # Retrieve the corresponding texts\n",
    "    retrieved_texts = [texts[i] for i in I[0]]\n",
    "    \n",
    "    return \" \".join(retrieved_texts)\n",
    "\n",
    "def generate_response(query, max_length=100):\n",
    "    # Retrieve relevant context\n",
    "    context = retrieve_context(query)\n",
    "    \n",
    "    # Create a prompt\n",
    "    prompt = f\"Context: {context}\\n\\nQuery: {query}\\n\\nResponse:\"\n",
    "    \n",
    "    # Generate response\n",
    "    response = generator(prompt, max_length=max_length, num_return_sequences=1)[0]['generated_text']\n",
    "    \n",
    "    return response.split(\"Response:\")[-1].strip()\n",
    "\n",
    "# Example usage\n",
    "query = \"What are Elon Musk's thoughts on AI?\"\n",
    "response = generate_response(query)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b2f1db-7339-4ebc-b703-622f1ff9d063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
