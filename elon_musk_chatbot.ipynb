{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e9847a-3b5b-45f2-9677-aa9de03553b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of sites used for models\n",
    "#https://sbert.net/\n",
    "#https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "#https://huggingface.co/openai/whisper-large-v3\n",
    "#https://pypi.org/project/faiss-cpu/\n",
    "\n",
    "#Text generation options: \n",
    "#https://huggingface.co/models?pipeline_tag=text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd1cc23-e77c-4ea1-9a4e-949353e2bb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2227"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the list from the JSON file\n",
    "with open('speech_total.json', 'r') as file:\n",
    "    speech_total = json.load(file)\n",
    "\n",
    "len(speech_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcabcbc4-bc71-4005-bdf7-b995c389e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Groq API\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API\"))\n",
    "\n",
    "def get_groq_response(query):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "        model=\"mixtral-8x7b-32768\",\n",
    "        temperature=0.5,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86e6b597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandonamarasingam/anaconda3/envs/pytorch/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/brandonamarasingam/anaconda3/envs/pytorch/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/brandonamarasingam/anaconda3/envs/pytorch/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: (__ZN3c1017RegisterOperatorsD1Ev)\n",
      "  Referenced from: '/Users/brandonamarasingam/anaconda3/envs/pytorch/lib/python3.11/site-packages/torchvision/image.so'\n",
      "  Expected in: '/Users/brandonamarasingam/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/Users/brandonamarasingam/anaconda3/envs/pytorch/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33abba1d2fd841e0b05cd76b53b21ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (2227, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandonamarasingam/anaconda3/envs/pytorch/lib/python3.11/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f353c5ccc58546f5a245fcfc398688b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1278 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 3.0438184663536777e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 1.0876369327073553e-05, 'epoch': 2.35}\n",
      "{'train_runtime': 1320.9751, 'train_samples_per_second': 3.868, 'train_steps_per_second': 0.967, 'train_loss': 0.0, 'epoch': 3.0}\n",
      "Error during generation: probability tensor contains either `inf`, `nan` or element < 0\n",
      "Query: What are your thoughts on AI?\n",
      "Response: I apologize, but I couldn't generate a response at this time.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import faiss\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the embedding model and create embeddings\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = embed_model.encode(speech_total, show_progress_bar=True)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Step 2: Create and save FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings.astype('float32'))\n",
    "faiss.write_index(index, \"faiss_index.bin\")\n",
    "with open('original_texts.pkl', 'wb') as f:\n",
    "    pickle.dump(speech_total, f)\n",
    "\n",
    "# Step 3: Prepare data for fine-tuning\n",
    "def prepare_data_for_finetuning(speeches):\n",
    "    with open('elon_speeches.txt', 'w', encoding='utf-8') as f:\n",
    "        for speech in speeches:\n",
    "            f.write(speech + '\\n\\n')\n",
    "\n",
    "prepare_data_for_finetuning(speech_total)\n",
    "\n",
    "# Step 4: Load the pre-trained model and tokenizer\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set the pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Step 5: Prepare the dataset\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"elon_speeches.txt\",\n",
    "    block_size=128\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Step 6: Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./elon_gpt2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3, # Can increase epoch number to improve training\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Step 7: Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Step 8: Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 9: Save the fine-tuned model\n",
    "trainer.save_model()\n",
    "\n",
    "# Step 10: Load the fine-tuned model for inference\n",
    "fine_tuned_model = GPT2LMHeadModel.from_pretrained(\"./elon_gpt2\")\n",
    "fine_tuned_model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Function to retrieve context\n",
    "def retrieve_context(query, k=3):\n",
    "    query_embedding = embed_model.encode([query])[0]\n",
    "    D, I = index.search(np.array([query_embedding]).astype('float32'), k)\n",
    "    retrieved_texts = [speech_total[i] for i in I[0]]\n",
    "    return \" \".join(retrieved_texts)\n",
    "\n",
    "# Updated elonify function\n",
    "def elonify(content):\n",
    "    prompt = f\"Elon Musk's response to '{content}': \"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)\n",
    "    input_ids = inputs['input_ids'].to(fine_tuned_model.device)\n",
    "    attention_mask = inputs['attention_mask'].to(fine_tuned_model.device)\n",
    "    \n",
    "    try:\n",
    "        output = fine_tuned_model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=200,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            temperature=0.2,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            top_k=50,  # Add top_k sampling\n",
    "            top_p=0.95,  # Add top_p (nucleus) sampling\n",
    "            repetition_penalty=1.2,  # Add repetition penalty\n",
    "            bad_words_ids=[[tokenizer.unk_token_id]]  # Prevent generation of unknown tokens\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error during generation: {e}\")\n",
    "        return \"I apologize, but I couldn't generate a response at this time.\"\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)[len(prompt):]\n",
    "\n",
    "\n",
    "# Updated generate_response function\n",
    "def generate_response(query, max_new_tokens=200):\n",
    "    # Retrieve context from your existing data\n",
    "    context = retrieve_context(query)\n",
    "    \n",
    "    # Combine query with retrieved context\n",
    "    combined_input = f\"Context: {context}\\n\\nQuery: {query}\\n\\nResponse:\"\n",
    "    \n",
    "    # Use the elonify function to generate response\n",
    "    response = elonify(combined_input)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "query = \"What are your thoughts on AI?\"\n",
    "response = generate_response(query)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "847a80a2-496c-4bb7-a606-e336d072a29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your question and press Enter to speak with.\n",
      "\n",
      "Q: what is the nba\n",
      "Error during generation: probability tensor contains either `inf`, `nan` or element < 0\n",
      "\n",
      "A: I apologize, but I couldn't generate a response at this time.\n",
      "\n",
      "Thank you for using ElonAI, goodbye!\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def interactive_qa():\n",
    "    while True:\n",
    "        query = input(\"Enter your question (or type 'exit' to quit): \")\n",
    "        if query.lower() == 'exit':\n",
    "            print(\"Thank you for using ElonAI, goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if query.strip():\n",
    "            print(f\"\\nQ: {query}\")\n",
    "            try:\n",
    "                response = generate_response(query)\n",
    "                print(f\"\\nA: {response}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {str(e)}\\n\")\n",
    "        else:\n",
    "            print(\"Please enter a question.\\n\")\n",
    "\n",
    "print(\"Type your question and press Enter to speak with.\")\n",
    "interactive_qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1db2e924-f1f2-4f5f-8edc-7c10a23d5380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during generation: probability tensor contains either `inf`, `nan` or element < 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during generation: probability tensor contains either `inf`, `nan` or element < 0\n",
      "Evaluated 2 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing and Improving RAG System\n",
    "\n",
    "import random # Used for sampling\n",
    "from tqdm import tqdm # Provides progress bars\n",
    "\n",
    "def evaluate_rag_system(num_samples=100):\n",
    "    # Sample queries or create a test set\n",
    "    test_queries = [\n",
    "        \"What does Elon Musk think about renewable energy?\",\n",
    "        \"How does Elon Musk view the future of space exploration?\",\n",
    "        # Add more diverse queries here\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for query in tqdm(random.sample(test_queries, min(num_samples, len(test_queries)))):\n",
    "        response = generate_response(query)\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            # You might add more metrics here, like response time, etc.\n",
    "        })\n",
    "\n",
    "    # Here you would typically add code to calculate metrics\n",
    "    # such as relevance, coherence, factual accuracy, etc.\n",
    "    # This often requires human evaluation or comparison against known ground truths\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = evaluate_rag_system()\n",
    "print(f\"Evaluated {len(eval_results)} queries\")\n",
    "# Add code here to analyze and display results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
