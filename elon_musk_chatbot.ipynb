{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e9847a-3b5b-45f2-9677-aa9de03553b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of sites used for models\n",
    "#https://sbert.net/\n",
    "#https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "#https://huggingface.co/openai/whisper-large-v3\n",
    "#https://pypi.org/project/faiss-cpu/\n",
    "\n",
    "#Text generation options: \n",
    "#https://huggingface.co/models?pipeline_tag=text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd1cc23-e77c-4ea1-9a4e-949353e2bb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2227"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the list from the JSON file\n",
    "with open('speech_total.json', 'r') as file:\n",
    "    speech_total = json.load(file)\n",
    "\n",
    "len(speech_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcabcbc4-bc71-4005-bdf7-b995c389e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Groq API\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API\"))\n",
    "\n",
    "def get_groq_response(query):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "        model=\"mixtral-8x7b-32768\",\n",
    "        temperature=0.5,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86e6b597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandonamarasingam/anaconda3/envs/pytorch/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/brandonamarasingam/anaconda3/envs/pytorch/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/brandonamarasingam/anaconda3/envs/pytorch/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: (__ZN3c1017RegisterOperatorsD1Ev)\n",
      "  Referenced from: '/Users/brandonamarasingam/anaconda3/envs/pytorch/lib/python3.11/site-packages/torchvision/image.so'\n",
      "  Expected in: '/Users/brandonamarasingam/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/Users/brandonamarasingam/anaconda3/envs/pytorch/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2ec641ab884861ac3cea38448d56f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (2227, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandonamarasingam/anaconda3/envs/pytorch/lib/python3.11/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8738777d278c4dbfb72b629d3bcc0da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1278 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 3.0438184663536777e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 1.0876369327073553e-05, 'epoch': 2.35}\n",
      "{'train_runtime': 1283.8796, 'train_samples_per_second': 3.979, 'train_steps_per_second': 0.995, 'train_loss': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 122\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m    121\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are your thoughts on AI?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 122\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 116\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(query, max_new_tokens)\u001b[0m\n\u001b[1;32m    113\u001b[0m combined_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResponse:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Use the elonify function to generate response\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43melonify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "Cell \u001b[0;32mIn[4], line 92\u001b[0m, in \u001b[0;36melonify\u001b[0;34m(content)\u001b[0m\n\u001b[1;32m     89\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(fine_tuned_model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     90\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(fine_tuned_model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 92\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfine_tuned_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add this line\u001b[39;49;00m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# top_k=50, limits vocab\u001b[39;49;00m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# top_p=0.95 nucleus sampling\u001b[39;49;00m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;28mlen\u001b[39m(prompt):]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/transformers/generation/utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/transformers/generation/utils.py:3020\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   3018\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3019\u001b[0m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[0;32m-> 3020\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3021\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3022\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import faiss\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the embedding model and create embeddings\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = embed_model.encode(speech_total, show_progress_bar=True)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Step 2: Create and save FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings.astype('float32'))\n",
    "faiss.write_index(index, \"faiss_index.bin\")\n",
    "with open('original_texts.pkl', 'wb') as f:\n",
    "    pickle.dump(speech_total, f)\n",
    "\n",
    "# Step 3: Prepare data for fine-tuning\n",
    "def prepare_data_for_finetuning(speeches):\n",
    "    with open('elon_speeches.txt', 'w', encoding='utf-8') as f:\n",
    "        for speech in speeches:\n",
    "            f.write(speech + '\\n\\n')\n",
    "\n",
    "prepare_data_for_finetuning(speech_total)\n",
    "\n",
    "# Step 4: Load the pre-trained model and tokenizer\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set the pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Step 5: Prepare the dataset\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"elon_speeches.txt\",\n",
    "    block_size=128\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Step 6: Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./elon_gpt2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3, # Can increase epoch number to improve training\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Step 7: Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Step 8: Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 9: Save the fine-tuned model\n",
    "trainer.save_model()\n",
    "\n",
    "# Step 10: Load the fine-tuned model for inference\n",
    "fine_tuned_model = GPT2LMHeadModel.from_pretrained(\"./elon_gpt2\")\n",
    "fine_tuned_model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Function to retrieve context\n",
    "def retrieve_context(query, k=3):\n",
    "    query_embedding = embed_model.encode([query])[0]\n",
    "    D, I = index.search(np.array([query_embedding]).astype('float32'), k)\n",
    "    retrieved_texts = [speech_total[i] for i in I[0]]\n",
    "    return \" \".join(retrieved_texts)\n",
    "\n",
    "# Updated elonify function\n",
    "def elonify(content):\n",
    "    prompt = f\"Elon Musk's response to '{content}': \"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)\n",
    "    input_ids = inputs['input_ids'].to(fine_tuned_model.device)\n",
    "    attention_mask = inputs['attention_mask'].to(fine_tuned_model.device)\n",
    "    \n",
    "    try:\n",
    "        output = fine_tuned_model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=200,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            top_k=50,  # Add top_k sampling\n",
    "            top_p=0.95,  # Add top_p (nucleus) sampling\n",
    "            repetition_penalty=1.2,  # Add repetition penalty\n",
    "            bad_words_ids=[[tokenizer.unk_token_id]]  # Prevent generation of unknown tokens\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error during generation: {e}\")\n",
    "        return \"I apologize, but I couldn't generate a response at this time.\"\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)[len(prompt):]\n",
    "\n",
    "\n",
    "# Updated generate_response function\n",
    "def generate_response(query, max_new_tokens=200):\n",
    "    # Retrieve context from your existing data\n",
    "    context = retrieve_context(query)\n",
    "    \n",
    "    # Combine query with retrieved context\n",
    "    combined_input = f\"Context: {context}\\n\\nQuery: {query}\\n\\nResponse:\"\n",
    "    \n",
    "    # Use the elonify function to generate response\n",
    "    response = elonify(combined_input)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "query = \"What are your thoughts on AI?\"\n",
    "response = generate_response(query)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847a80a2-496c-4bb7-a606-e336d072a29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your question and press Enter to speak with.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your question (or type 'exit' to quit):  what is the nba\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: what is the nba\n",
      "\n",
      "A: !\"!#!$!%!&!'!(!)!*!+!,!-!.!/!0!1!2!3!4!5!6!7!8!9!:!;!<!=!>!?!@!A!B!C!D!E!F!G!H!I!J!K!L!M!N!O!P!Q!R!S!T!U!V!W!X!Y!Z![!\\!]!^!_!`!a!b!c!d!e!f!g!h!i!j!k!l!m!n!o!p!q!r!s!t!u!v!w!x!y!z!{!|!}!~!ï¿½!ï¿½!ï¿½!ï¿½!ï¿½!ï¿½!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your question (or type 'exit' to quit):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for using ElonAI, goodbye!\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def interactive_qa():\n",
    "    while True:\n",
    "        query = input(\"Enter your question (or type 'exit' to quit): \")\n",
    "        if query.lower() == 'exit':\n",
    "            print(\"Thank you for using ElonAI, goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if query.strip():\n",
    "            print(f\"\\nQ: {query}\")\n",
    "            try:\n",
    "                response = generate_response(query)\n",
    "                print(f\"\\nA: {response}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {str(e)}\\n\")\n",
    "        else:\n",
    "            print(\"Please enter a question.\\n\")\n",
    "\n",
    "print(\"Type your question and press Enter to speak with.\")\n",
    "interactive_qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120436e7-94f3-4ee5-957f-25535cf15b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2e924-f1f2-4f5f-8edc-7c10a23d5380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing and Improving RAG System\n",
    "\n",
    "import random # Used for sampling\n",
    "from tqdm import tqdm # Provides progress bars\n",
    "\n",
    "def evaluate_rag_system(num_samples=100):\n",
    "    # Sample queries or create a test set\n",
    "    test_queries = [\n",
    "        \"What does Elon Musk think about renewable energy?\",\n",
    "        \"How does Elon Musk view the future of space exploration?\",\n",
    "        # Add more diverse queries here\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for query in tqdm(random.sample(test_queries, min(num_samples, len(test_queries)))):\n",
    "        response = generate_response(query)\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            # You might add more metrics here, like response time, etc.\n",
    "        })\n",
    "\n",
    "    # Here you would typically add code to calculate metrics\n",
    "    # such as relevance, coherence, factual accuracy, etc.\n",
    "    # This often requires human evaluation or comparison against known ground truths\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = evaluate_rag_system()\n",
    "print(f\"Evaluated {len(eval_results)} queries\")\n",
    "# Add code here to analyze and display results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
