{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the list from the JSON file\n",
    "with open('speech_total.json', 'r') as file:\n",
    "    speech_total = json.load(file)\n",
    "\n",
    "len(speech_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Groq API\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API\"))\n",
    "\n",
    "def get_groq_response(query):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "        model=\"mixtral-8x7b-32768\",\n",
    "        temperature=0.5,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import faiss\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the embedding model\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embedding\n",
    "embeddings = embed_model.encode(speech_total, show_progress_bar=True)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Step 2: Create and save FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings.astype('float32'))\n",
    "\n",
    "# Save the index and texts\n",
    "faiss.write_index(index, \"faiss_index.bin\")\n",
    "with open('original_texts.pkl', 'wb') as f:\n",
    "    pickle.dump(speech_total, f)\n",
    "\n",
    "# Step 3: Load the language model and tokenizer\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set the pad token to be the end of sequence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Check if GPU is available, and if so set up device for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# Create function to retrieve similar context from FAISS index based on query given\n",
    "def retrieve_context(query, k=3):\n",
    "    query_embedding = embed_model.encode([query])[0]\n",
    "    D, I = index.search(np.array([query_embedding]).astype('float32'), k)\n",
    "    retrieved_texts = [speech_total[i] for i in I[0]]\n",
    "    return \" \".join(retrieved_texts)\n",
    "\n",
    "# Create function to generate a response, by combining retrieved context and response from Groq, then making it sound like Elon\n",
    "def generate_response(query, max_new_tokens=200):\n",
    "    # Get factual response from Groq\n",
    "    groq_response = get_groq_response(query)\n",
    "    \n",
    "    # Retrieve context from your existing data\n",
    "    context = retrieve_context(query)\n",
    "    \n",
    "    # Combine Groq response with retrieved context\n",
    "    combined_input = f\"Context: {context}\\n\\nGroq Response: {groq_response}\\n\\nQuery: {query}\\n\\nResponse:\"\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    \n",
    "    # Generate Elon-style response\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    generated_text = response.split(\"Response:\")[-1].strip()\n",
    "    \n",
    "    # Call Elonify (function below) to make response sound like Elon\n",
    "    return elonify(generated_text)\n",
    "\n",
    "# Function to make response sound like Elon\n",
    "def elonify(content):\n",
    "    # Ask our trained language model to rewrite following answer into Elon sounding answer\n",
    "    prompt = f\"\"\"\n",
    "    Rewrite the following content in Elon Musk's speaking style, based on the training data:\n",
    "\n",
    "    {content}\n",
    "\n",
    "    Elon Musk's response:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=1000) # If desire longer input, change max_length\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=1000, # Token size for answer\n",
    "        num_return_sequences=1, # Number of responses\n",
    "        no_repeat_ngram_size=2, # Max 2 consecutive words\n",
    "        temperature=0.7, # Randomness\n",
    "        pad_token_id=tokenizer.eos_token_id # Set padding token to end of sequence token\n",
    "    )\n",
    "    \n",
    "    # Convert model output back into text, takes response\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True).split(\"Elon Musk's response:\")[-1].strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing and Improving RAG System\n",
    "\n",
    "import random # Used for sampling\n",
    "from tqdm import tqdm # Provides progress bars\n",
    "\n",
    "def evaluate_rag_system(num_samples=100):\n",
    "    # Sample queries or create a test set\n",
    "    test_queries = [\n",
    "        \"What does Elon Musk think about renewable energy?\",\n",
    "        \"How does Elon Musk view the future of space exploration?\",\n",
    "        # Add more diverse queries here\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for query in tqdm(random.sample(test_queries, min(num_samples, len(test_queries)))):\n",
    "        response = generate_response(query)\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            # You might add more metrics here, like response time, etc.\n",
    "        })\n",
    "\n",
    "    # Here you would typically add code to calculate metrics\n",
    "    # such as relevance, coherence, factual accuracy, etc.\n",
    "    # This often requires human evaluation or comparison against known ground truths\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = evaluate_rag_system()\n",
    "print(f\"Evaluated {len(eval_results)} queries\")\n",
    "# Add code here to analyze and display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def interactive_qa():\n",
    "    while True:\n",
    "        query = input(\"Enter your question (or type 'exit' to quit): \")\n",
    "        if query.lower() == 'exit':\n",
    "            print(\"Thank you for using ElonAI, goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if query.strip():\n",
    "            print(f\"\\nQ: {query}\")\n",
    "            try:\n",
    "                response = generate_response(query)\n",
    "                print(f\"\\nA: {response}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {str(e)}\\n\")\n",
    "        else:\n",
    "            print(\"Please enter a question.\\n\")\n",
    "\n",
    "print(\"Type your question and press Enter to speak with.\")\n",
    "interactive_qa()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
