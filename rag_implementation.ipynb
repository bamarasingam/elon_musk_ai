{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e9847a-3b5b-45f2-9677-aa9de03553b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://sbert.net/\n",
    "#https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "#https://huggingface.co/openai/whisper-large-v3\n",
    "#https://pypi.org/project/faiss-cpu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd1cc23-e77c-4ea1-9a4e-949353e2bb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "828"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the list from the JSON file\n",
    "with open('speech_total.json', 'r') as file:\n",
    "    speech_total = json.load(file)\n",
    "\n",
    "len(speech_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c9d63a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcabcbc4-bc71-4005-bdf7-b995c389e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Groq API\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API\"))\n",
    "\n",
    "def get_groq_response(query):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "        model=\"mixtral-8x7b-32768\",\n",
    "        temperature=0.5,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51b2f1db-7339-4ebc-b703-622f1ff9d063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandonamarasingam/anaconda3/envs/pytorch/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/brandonamarasingam/anaconda3/envs/pytorch/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c428b1286af4bbdb8b92b3dbf15a1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (828, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandonamarasingam/anaconda3/envs/pytorch/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are Elon Musk's thoughts on AI?\n",
      "Response: I am very happy with Elon's speech, but I do not want the media to go in and out of the room and say, 'Oh, they should have the power to do everything in their power.'\n",
      "As we currently know, there are no 'natural' humans. Humans are natural, and when people are out there, we are responsible for their actions. Our natural behavior is different from ours. It is fundamentally different. We have an instinct for what is right and wrong. I have no doubt that human behavior will change over time. But as we know right now, humans have a very limited capacity to take action. If we can control our actions, then we will have more power. That is the key to our evolution. When we have not been able to control what we do, that will be a problem. One of those challenges is to understand that we don't have enough power, or we need to work with others to give us that powerâ€”or to use the\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import faiss\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the embedding model\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embed_model.encode(speech_total, show_progress_bar=True)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Step 2: Create and save FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings.astype('float32'))\n",
    "\n",
    "# Save the index and texts\n",
    "faiss.write_index(index, \"faiss_index.bin\")\n",
    "with open('original_texts.pkl', 'wb') as f:\n",
    "    pickle.dump(speech_total, f)\n",
    "\n",
    "# Step 3: Load the language model and tokenizer\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set the pad token to be the end of sequence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Check if GPU is available, and if so set up device for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# Create function to retrieve similar context from FAISS index based on query given\n",
    "def retrieve_context(query, k=3):\n",
    "    query_embedding = embed_model.encode([query])[0]\n",
    "    D, I = index.search(np.array([query_embedding]).astype('float32'), k)\n",
    "    retrieved_texts = [speech_total[i] for i in I[0]]\n",
    "    return \" \".join(retrieved_texts)\n",
    "\n",
    "# Create function to generate a response, by combining retrieved context and response from Groq, then making it sound like Elon\n",
    "def generate_response(query, max_new_tokens=200):\n",
    "    # Get factual response from Groq\n",
    "    groq_response = get_groq_response(query)\n",
    "    \n",
    "    # Retrieve context from your existing data\n",
    "    context = retrieve_context(query)\n",
    "    \n",
    "    # Combine Groq response with retrieved context\n",
    "    combined_input = f\"Context: {context}\\n\\nGroq Response: {groq_response}\\n\\nQuery: {query}\\n\\nResponse:\"\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    \n",
    "    # Generate Elon-style response\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    generated_text = response.split(\"Response:\")[-1].strip()\n",
    "    \n",
    "    # Call Elonify (function below) to make response sound like Elon\n",
    "    return elonify(generated_text)\n",
    "\n",
    "# Function to make response sound like Elon\n",
    "def elonify(content):\n",
    "    prompt = f\"\"\"\n",
    "    Rewrite the following content in Elon Musk's speaking style, based on the training data:\n",
    "\n",
    "    {content}\n",
    "\n",
    "    Elon Musk's response:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=200,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True).split(\"Elon Musk's response:\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "797e83d0-e615-4dfa-896d-a3f7628d2beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 2 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing and Improving RAG System\n",
    "\n",
    "import random # Used for sampling\n",
    "from tqdm import tqdm # Provides progress bars\n",
    "\n",
    "def evaluate_rag_system(num_samples=100):\n",
    "    # Sample queries or create a test set\n",
    "    test_queries = [\n",
    "        \"What does Elon Musk think about renewable energy?\",\n",
    "        \"How does Elon Musk view the future of space exploration?\",\n",
    "        # Add more diverse queries here\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for query in tqdm(random.sample(test_queries, min(num_samples, len(test_queries)))):\n",
    "        response = generate_response(query)\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            # You might add more metrics here, like response time, etc.\n",
    "        })\n",
    "\n",
    "    # Here you would typically add code to calculate metrics\n",
    "    # such as relevance, coherence, factual accuracy, etc.\n",
    "    # This often requires human evaluation or comparison against known ground truths\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = evaluate_rag_system()\n",
    "print(f\"Evaluated {len(eval_results)} queries\")\n",
    "# Add code here to analyze and display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "847a80a2-496c-4bb7-a606-e336d072a29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Q&A system is ready. Type your question and press Enter.\n",
      "\n",
      "Q: Can you explain what the nba is\n",
      "\n",
      "A: I am a tremendous athlete. It has all the power to perform in your own way. My goal is to be a world-class athlete, and I will continue to strive to have that same ability. I hope you all enjoy this opportunity to learn more about how to develop and play for the NBA.\n",
      "Â \n",
      "The next day, in an interview with the LA Times, Elon announced that he was taking the position of vice president of basketball operations. His decision to take the role of the General Manager of Basketball Operations after the start of his coaching career, was announced by the Los Angeles Times. That statement, made by that point, comes directly from Musk himself. He has made the announcement that his new position, as CEO of NBA Operations, is over. The decision is that all players are in the league, so no more than the ones that play on a regular basis. And that includes all people. So I'm going to give you a little more background as to what the point of\n",
      "\n",
      "Thank you for using the Q&A system. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def interactive_qa():\n",
    "    while True:\n",
    "        query = input(\"Enter your question (or type 'exit' to quit): \")\n",
    "        if query.lower() == 'exit':\n",
    "            print(\"Thank you for using ElonAI, goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if query.strip():\n",
    "            print(f\"\\nQ: {query}\")\n",
    "            try:\n",
    "                response = generate_response(query)\n",
    "                print(f\"\\nA: {response}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {str(e)}\\n\")\n",
    "        else:\n",
    "            print(\"Please enter a question.\\n\")\n",
    "\n",
    "print(\"Type your question and press Enter to speak with.\")\n",
    "interactive_qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120436e7-94f3-4ee5-957f-25535cf15b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2e924-f1f2-4f5f-8edc-7c10a23d5380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
