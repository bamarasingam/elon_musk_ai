{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39e9847a-3b5b-45f2-9677-aa9de03553b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://sbert.net/\n",
    "#https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "#https://huggingface.co/openai/whisper-large-v3\n",
    "#https://pypi.org/project/faiss-cpu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdd1cc23-e77c-4ea1-9a4e-949353e2bb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "828"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the list from the JSON file\n",
    "with open('speech_total.json', 'r') as file:\n",
    "    speech_total = json.load(file)\n",
    "\n",
    "len(speech_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51b2f1db-7339-4ebc-b703-622f1ff9d063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████████████████████████████████████████████| 26/26 [00:03<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (828, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are Elon Musk's thoughts on AI?\n",
      "Response: Well I‪m not sure about Elon, but I am sure he has some thoughts. Elon is going to say something. But I don‖m sure that Elon has a lot of ideas about how to best manage a society and how we\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import faiss\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the embedding model and create embeddings\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embed_model.encode(speech_total, show_progress_bar=True)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Step 2: Create and save FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings.astype('float32'))\n",
    "\n",
    "# Save the index and texts\n",
    "faiss.write_index(index, \"faiss_index.bin\")\n",
    "with open('original_texts.pkl', 'wb') as f:\n",
    "    pickle.dump(speech_total, f)\n",
    "\n",
    "# Step 3: Load the language model and tokenizer\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "def retrieve_context(query, k=3):\n",
    "    query_embedding = embed_model.encode([query])[0]\n",
    "    D, I = index.search(np.array([query_embedding]).astype('float32'), k)\n",
    "    retrieved_texts = [speech_total[i] for i in I[0]]\n",
    "    return \" \".join(retrieved_texts)\n",
    "\n",
    "def generate_response(query, max_new_tokens=50):\n",
    "    context = retrieve_context(query)\n",
    "    prompt = f\"Context: {context}\\n\\nQuery: {query}\\n\\nResponse:\"\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate the response\n",
    "    output = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the generated part\n",
    "    generated_text = response.split(\"Response:\")[-1].strip()\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "query = \"What are Elon Musk's thoughts on AI?\"\n",
    "response = generate_response(query)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "797e83d0-e615-4dfa-896d-a3f7628d2beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                               | 0/2 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|███████████████████████████████████████████▌                                           | 1/2 [00:09<00:09,  9.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 2 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_rag_system(num_samples=100):\n",
    "    # Sample queries or create a test set\n",
    "    test_queries = [\n",
    "        \"What does Elon Musk think about renewable energy?\",\n",
    "        \"How does Elon Musk view the future of space exploration?\",\n",
    "        # Add more diverse queries here\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for query in tqdm(random.sample(test_queries, min(num_samples, len(test_queries)))):\n",
    "        response = generate_response(query)\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            # You might add more metrics here, like response time, etc.\n",
    "        })\n",
    "\n",
    "    # Here you would typically add code to calculate metrics\n",
    "    # such as relevance, coherence, factual accuracy, etc.\n",
    "    # This often requires human evaluation or comparison against known ground truths\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = evaluate_rag_system()\n",
    "print(f\"Evaluated {len(eval_results)} queries\")\n",
    "# Add code here to analyze and display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "847a80a2-496c-4bb7-a606-e336d072a29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Q&A system is ready. Type your question and press Enter.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question (or type 'exit' to quit):  Can you explain what SpaceX is to me\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: Can you explain what SpaceX is to me\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A: Yeah. I mean, you‪ve been here for a while and I am a huge fan of SpaceX, and you talked about the need to get people into space and get them into the business of getting astronauts and then making them in the U\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question (or type 'exit' to quit):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for using the Q&A system. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def interactive_qa():\n",
    "    while True:\n",
    "        query = input(\"Enter your question (or type 'exit' to quit): \")\n",
    "        if query.lower() == 'exit':\n",
    "            print(\"Thank you for using the Q&A system. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if query.strip():\n",
    "            print(f\"\\nQ: {query}\")\n",
    "            try:\n",
    "                response = generate_response(query)\n",
    "                print(f\"\\nA: {response}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {str(e)}\\n\")\n",
    "        else:\n",
    "            print(\"Please enter a question.\\n\")\n",
    "        \n",
    "        # Optional: Uncomment the next line if you want to clear output after each Q&A\n",
    "        # clear_output(wait=True)\n",
    "\n",
    "print(\"The Q&A system is ready. Type your question and press Enter.\")\n",
    "interactive_qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120436e7-94f3-4ee5-957f-25535cf15b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5e0f20-48d8-4f14-a57f-40aca8dac2a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
