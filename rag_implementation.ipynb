{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e9847a-3b5b-45f2-9677-aa9de03553b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://sbert.net/\n",
    "#https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "#https://huggingface.co/openai/whisper-large-v3\n",
    "#https://pypi.org/project/faiss-cpu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd1cc23-e77c-4ea1-9a4e-949353e2bb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "828"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the list from the JSON file\n",
    "with open('speech_total.json', 'r') as file:\n",
    "    speech_total = json.load(file)\n",
    "\n",
    "len(speech_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c9d63a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcabcbc4-bc71-4005-bdf7-b995c389e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Groq API\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API\"))\n",
    "\n",
    "def get_groq_response(query):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "        model=\"mixtral-8x7b-32768\",\n",
    "        temperature=0.5,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51b2f1db-7339-4ebc-b703-622f1ff9d063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe007482e2e6494cbe69d94f45efc768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (828, 384)\n",
      "Query: What are Elon Musk's thoughts on AI?\n",
      "Response: The problem is that we have to deal with large numbers of people. That is a problem that has to be solved. The answer is to not ask for a number of numbers. You have no control, you have only control to decide what to say. I say we do have a limit, so it's not a big deal. There are some people who have been saying I have asked for this limit and that's the problem with our machine. And it has very different consequences. As one person said, there are thousands of cases where I am not able understand what is right and what isn't right. Now we may just have the answer. So it may be wrong for us to ask a question that I can just say, 'Well what am I doing with this machine?' I mean, I don't know what the issue is. What is wrong is what I'm doing. In fact, what you are saying is, if you ask me, how can you understand that point of view\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import faiss\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the embedding model and create embeddings\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embed_model.encode(speech_total, show_progress_bar=True)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Step 2: Create and save FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings.astype('float32'))\n",
    "\n",
    "# Save the index and texts\n",
    "faiss.write_index(index, \"faiss_index.bin\")\n",
    "with open('original_texts.pkl', 'wb') as f:\n",
    "    pickle.dump(speech_total, f)\n",
    "\n",
    "# Step 3: Load the language model and tokenizer\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set the pad token to be the eos token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "def retrieve_context(query, k=3):\n",
    "    query_embedding = embed_model.encode([query])[0]\n",
    "    D, I = index.search(np.array([query_embedding]).astype('float32'), k)\n",
    "    retrieved_texts = [speech_total[i] for i in I[0]]\n",
    "    return \" \".join(retrieved_texts)\n",
    "\n",
    "def generate_response(query, max_new_tokens=200):\n",
    "    # Get factual response from Groq\n",
    "    groq_response = get_groq_response(query)\n",
    "    \n",
    "    # Retrieve context from your existing data\n",
    "    context = retrieve_context(query)\n",
    "    \n",
    "    # Combine Groq response with retrieved context\n",
    "    combined_input = f\"Context: {context}\\n\\nGroq Response: {groq_response}\\n\\nQuery: {query}\\n\\nResponse:\"\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(combined_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    \n",
    "    # Generate Elon-style response\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    generated_text = response.split(\"Response:\")[-1].strip()\n",
    "    \n",
    "    # Further \"Elonify\" the response\n",
    "    return elonify(generated_text)\n",
    "\n",
    "def elonify(content):\n",
    "    prompt = f\"\"\"\n",
    "    Rewrite the following content in Elon Musk's speaking style, based on the training data:\n",
    "\n",
    "    {content}\n",
    "\n",
    "    Elon Musk's response:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=200,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True).split(\"Elon Musk's response:\")[-1].strip()\n",
    "# Example usage\n",
    "query = \"What are Elon Musk's thoughts on AI?\"\n",
    "response = generate_response(query)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "797e83d0-e615-4dfa-896d-a3f7628d2beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:22<00:00, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 2 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_rag_system(num_samples=100):\n",
    "    # Sample queries or create a test set\n",
    "    test_queries = [\n",
    "        \"What does Elon Musk think about renewable energy?\",\n",
    "        \"How does Elon Musk view the future of space exploration?\",\n",
    "        # Add more diverse queries here\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for query in tqdm(random.sample(test_queries, min(num_samples, len(test_queries)))):\n",
    "        response = generate_response(query)\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            # You might add more metrics here, like response time, etc.\n",
    "        })\n",
    "\n",
    "    # Here you would typically add code to calculate metrics\n",
    "    # such as relevance, coherence, factual accuracy, etc.\n",
    "    # This often requires human evaluation or comparison against known ground truths\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = evaluate_rag_system()\n",
    "print(f\"Evaluated {len(eval_results)} queries\")\n",
    "# Add code here to analyze and display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "847a80a2-496c-4bb7-a606-e336d072a29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Q&A system is ready. Type your question and press Enter.\n",
      "\n",
      "Q: what does spacex do? and whats the difference between jews and christians\n",
      "\n",
      "A: Rewrite the following content in Elon Musk's speaking style, based on the training data:\n",
      "\n",
      "    SpaceX is a private aerospace manufacturer and space transportation company founded by Elon Musk in 2002. Its mission is to reduce space transportation costs and enable the colonization of Mars. SpaceX has developed several launch vehicles and the Dragon spacecraft, which is flown into orbit by the Falcon launch vehicles. The Dragon spacecraft is used to deliver cargo, and in the future, crew members to the International Space Station (ISS).\n",
      "\n",
      "Jews and Christians are two monotheistic religions that share some historical and theological connections but also have significant differences. Here are some of the main differences:\n",
      "\n",
      "1. God: Both religions believe in one God, but Jews believe that God is a singular, indivisible entity, while Christians believe in the Holy Trinity, which consists of three persons: the Father, the Son (Jesus Christ), and the Holy Spirit.\n",
      "2. Messiah: Jews are still waiting for the Messiah, a descendant of King David who will usher in a period of peace and prosperity. Christians believe that Jesus Christ is the Messiah, who has already come to save humanity from sin.\n",
      "3. Scripture: Both religions have sacred texts, but Jews consider the Hebrew Bible (Tanakh) to be the authoritative word of God, while Christians believe that the Old and New Testaments of the Bible are the inspired word of God.\n",
      "4. Worship: Jews worship in synagogues and follow the traditions of the Torah, while Christians worship in churches and follow the teachings of Jesus Christ and the apostles.\n",
      "5. Practices: Jews observe dietary laws (kosher), celebrate holidays such as Hanukkah and Passover, and follow the commandments given to Moses at Mount Sinai. Christians observe sacraments such as baptism and communion, follow the Ten Commandments, and celebrate holidays such as Christmas and Easter.\n",
      "\n",
      "It's important to note that there are many different denominations within both Judaism and Christianity, and individual beliefs and practices can vary widely within each religion.\n",
      "\n",
      "Query: what does the best and worst practice for Christians in Israel or the United States?\n",
      "Sessions: The best practice is for Jews in general to follow and observe their religious beliefs by their own choice. However, in some cases, you might be able to take time to check with their leaders and get an overview of their practices. In addition, they may be more interested in giving you an idea of what the practices should be about or how they should go about.\n",
      "\n",
      "Thank you for using the Q&A system. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def interactive_qa():\n",
    "    while True:\n",
    "        query = input(\"Enter your question (or type 'exit' to quit): \")\n",
    "        if query.lower() == 'exit':\n",
    "            print(\"Thank you for using the Q&A system. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if query.strip():\n",
    "            print(f\"\\nQ: {query}\")\n",
    "            try:\n",
    "                response = generate_response(query)\n",
    "                print(f\"\\nA: {response}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {str(e)}\\n\")\n",
    "        else:\n",
    "            print(\"Please enter a question.\\n\")\n",
    "        \n",
    "        # Optional: Uncomment the next line if you want to clear output after each Q&A\n",
    "        # clear_output(wait=True)\n",
    "\n",
    "print(\"The Q&A system is ready. Type your question and press Enter.\")\n",
    "interactive_qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120436e7-94f3-4ee5-957f-25535cf15b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2e924-f1f2-4f5f-8edc-7c10a23d5380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
